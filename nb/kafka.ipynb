{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff81adea-ef68-4e86-8eee-fa9292fd2b71",
   "metadata": {},
   "source": [
    "# simran4@wisc.edu, rgundavarapu@wisc.edu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44722ef8-f67f-47c2-82b2-9d46450fad68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime, time, random, string\n",
    "\n",
    "def one_station(name):\n",
    "    # temp pattern\n",
    "    month_avg = [27,31,44,58,70,79,83,81,74,61,46,32]\n",
    "    shift = (random.random()-0.5) * 30\n",
    "    month_avg = [m + shift + (random.random()-0.5) * 5 for m in month_avg]\n",
    "    \n",
    "    # rain pattern\n",
    "    start_rain = [0.1,0.1,0.3,0.5,0.4,0.2,0.2,0.1,0.2,0.2,0.2,0.1]\n",
    "    shift = (random.random()-0.5) * 0.1\n",
    "    start_rain = [r + shift + (random.random() - 0.5) * 0.2 for r in start_rain]\n",
    "    stop_rain = 0.2 + random.random() * 0.2\n",
    "\n",
    "    # day's state\n",
    "    today = datetime.date(2000, 1, 1)\n",
    "    temp = month_avg[0]\n",
    "    raining = False\n",
    "    \n",
    "    # gen weather\n",
    "    while True:\n",
    "        # choose temp+rain\n",
    "        month = today.month - 1\n",
    "        temp = temp * 0.8 + month_avg[month] * 0.2 + (random.random()-0.5) * 20\n",
    "        if temp < 32:\n",
    "            raining=False\n",
    "        elif raining and random.random() < stop_rain:\n",
    "            raining = False\n",
    "        elif not raining and random.random() < start_rain[month]:\n",
    "            raining = True\n",
    "\n",
    "        yield (today.strftime(\"%Y-%m-%d\"), name, temp, raining)\n",
    "\n",
    "        # next day\n",
    "        today += datetime.timedelta(days=1)\n",
    "        \n",
    "def all_stations(count=10, sleep_sec=1):\n",
    "    assert count <= 26\n",
    "    stations = []\n",
    "    for name in string.ascii_uppercase[:count]:\n",
    "        stations.append(one_station(name))\n",
    "    while True:\n",
    "        for station in stations:\n",
    "            yield next(station)\n",
    "        time.sleep(sleep_sec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a617bb72-084c-46a2-a2df-1f041e36ec92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cannot delete (may not exist yet)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['stations-json', 'stations']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from kafka import KafkaAdminClient, KafkaProducer, KafkaConsumer, TopicPartition\n",
    "from kafka.admin import NewTopic\n",
    "from kafka.errors import TopicAlreadyExistsError, UnknownTopicOrPartitionError\n",
    "\n",
    "broker = \"kafka:9092\"\n",
    "admin = KafkaAdminClient(bootstrap_servers =[broker])\n",
    "try:\n",
    "    admin.delete_topics([\"stations\", \"stations-json\"])\n",
    "    print(\"deleted\")\n",
    "except UnknownTopicOrPartitionError:\n",
    "    print(\"cannot delete (may not exist yet)\")\n",
    "\n",
    "time.sleep(1)\n",
    "admin.create_topics([NewTopic(\"stations\", 6, 1)])\n",
    "admin.create_topics([NewTopic(\"stations-json\", 6, 1)])\n",
    "admin.list_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4de1c4fd-f121-437d-b8a6-949c0520550f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from message_pb2 import *\n",
    "from threading import Thread\n",
    "import threading,json\n",
    "def produce():\n",
    "    producer = KafkaProducer(bootstrap_servers=[\"kafka:9092\"],retries=10,acks=-1)\n",
    "    \n",
    "    for date, station, degrees, raining in all_stations(15):\n",
    "        #print(date, station, degrees, raining)\n",
    "        s = Report(date=date, station=station,degrees=(degrees),raining=raining)\n",
    "        value = s.SerializeToString()\n",
    "        key=station.encode('utf-8')\n",
    "        producer.send('stations',key=key ,value=value)\n",
    "        value_json = {\"date\": date, \"station\": station,'degrees':(degrees),'raining':int(raining)}\n",
    "        value_json = bytes(json.dumps(value_json), \"utf-8\")\n",
    "        producer.send('stations-json',key=key,value=value_json)\n",
    "        # TODO: send to \"stations\" stream using protobuf\n",
    "        # TODO: send to \"stations-json\" using JSON\n",
    "\n",
    "# TODO: start thread to run produce\n",
    "threading.Thread(target=produce).start()\n",
    "# never join thread because we want it to run forever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1a3a46-f8d2-4867-8fb4-38efecec8f40",
   "metadata": {},
   "source": [
    "## PART 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b559f96-7419-415c-85f7-efb05f9e34fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "\n",
    "for partition in range(6):\n",
    "    path = f\"partition-{partition}.json\"\n",
    "    if os.path.exists(path):\n",
    "        os.remove(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6931146-95a3-420c-ae4f-b8da3f02b1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_partition(partition_num):\n",
    "    path = f\"partition-{partition_num}.json\"\n",
    "    if os.path.exists(path):\n",
    "        with open(path, \"r\") as file:\n",
    "            return json.load(file)\n",
    "    else:\n",
    "        return {'partition':partition_num}\n",
    "\n",
    "def save_partition(partition):\n",
    "    path = f\"partition-{partition['partition']}.json\"\n",
    "    with open(path, \"w\") as file:\n",
    "        json.dump(partition, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82fbf320-4ccb-48de-8e6d-01fa7b57d332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUND 0\n",
      "fortnut\n",
      "exiting\n",
      "exiting\n",
      "exiting\n",
      "ROUND 1\n",
      "fortnut\n",
      "exiting\n",
      "exiting\n",
      "exiting\n"
     ]
    }
   ],
   "source": [
    "def consume(part_nums=[], iterations=10):\n",
    "    consumer = KafkaConsumer(bootstrap_servers=[broker])\n",
    "    # TODO: create list of TopicPartition objects\n",
    "    partition_objs=[TopicPartition('stations',part_num) for part_num in part_nums]\n",
    "    consumer.assign(partition_objs)\n",
    "    partitions={}\n",
    "    # PART 1: initialization\n",
    "    for part_num in part_nums:\n",
    "        partitions['partition']=load_partition(part_num)['partition'] # key=partition num, value=snapshot dict\n",
    "        if 'offset' in partitions.keys():\n",
    "            consumer.seek(TopicPartition('stations',part_num),partitions['offset'])\n",
    "        else:\n",
    "            consumer.seek_to_beginning(TopicPartition('stations',part_num)) # else\n",
    "        counter=1\n",
    "        for i in range(iterations):\n",
    "            batch = consumer.poll(1000) # 1s timeout\n",
    "            date_list=[]\n",
    "            for tp, messages in batch.items():\n",
    "                for msg in messages:\n",
    "                    s = Report.FromString(msg.value)\n",
    "                    date_list.append(s.date)\n",
    "                    #print(s.station)\n",
    "                    if s.station in partitions.keys():\n",
    "                        if s.date<=partitions[s.station]['end']:\n",
    "                            print('fortnut')\n",
    "                            break\n",
    "                            break\n",
    "                        partitions[s.station]['sum']+=s.degrees\n",
    "                        partitions[s.station]['count']+=1\n",
    "                        partitions[s.station]['avg']=(partitions[s.station]['sum']/counter)\n",
    "                        partitions[s.station]['start']=s.date if s.date<partitions[s.station]['start'] else partitions[s.station]['start']\n",
    "                        partitions[s.station]['end']=s.date\n",
    "                    else:\n",
    "                        partitions[s.station]={}\n",
    "                        partitions[s.station]['sum']=s.degrees\n",
    "                        partitions[s.station]['count']=0\n",
    "                        partitions[s.station]['avg']=0\n",
    "                        partitions[s.station]['start']=s.date\n",
    "                        partitions[s.station]['end']=s.date\n",
    "\n",
    "                    partitions['offset']=consumer.position(tp)\n",
    "                    save_partition(partitions)\n",
    "    print(\"exiting\")\n",
    "\n",
    "for i in range(2):\n",
    "    print(\"ROUND\", i)\n",
    "    t1 = threading.Thread(target=consume, args=([0,1], 30))\n",
    "    t2 = threading.Thread(target=consume, args=([2,3], 30))\n",
    "    t3 = threading.Thread(target=consume, args=([4,5], 30))\n",
    "    t1.start()\n",
    "    t2.start()\n",
    "    t3.start()\n",
    "    t1.join()\n",
    "    t2.join()\n",
    "    t3.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ab6fb81-9545-43d8-b93b-a52ac21a5f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"partition\": 2, \"F\": {\"sum\": 1812.4314752510593, \"count\": 74, \"avg\": 1812.4314752510593, \"start\": \"2000-01-01\", \"end\": \"2000-03-15\"}, \"offset\": 223, \"I\": {\"sum\": 2225.7466875256, \"count\": 73, \"avg\": 2225.7466875256, \"start\": \"2000-01-01\", \"end\": \"2000-03-14\"}, \"J\": {\"sum\": 2158.563633201066, \"count\": 73, \"avg\": 2158.563633201066, \"start\": \"2000-01-01\", \"end\": \"2000-03-14\"}, \"D\": {\"sum\": 1115.9907526147645, \"count\": 25, \"avg\": 1115.9907526147645, \"start\": \"2000-02-19\", \"end\": \"2000-03-15\"}, \"G\": {\"sum\": 869.5137816608352, \"count\": 24, \"avg\": 869.5137816608352, \"start\": \"2000-02-19\", \"end\": \"2000-03-14\"}, \"M\": {\"sum\": 869.3182081640207, \"count\": 24, \"avg\": 869.3182081640207, \"start\": \"2000-02-19\", \"end\": \"2000-03-14\"}}"
     ]
    }
   ],
   "source": [
    "!cat partition-2.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526e5e33-6373-4b42-b230-56637178f0f4",
   "metadata": {},
   "source": [
    "part_nums=[0,1]\n",
    "consumer = KafkaConsumer(bootstrap_servers=[broker])\n",
    "# TODO: create list of TopicPartition objects\n",
    "partition_objs=[TopicPartition('stations',part_num) for part_num in part_nums]\n",
    "consumer.assign(partition_objs)\n",
    "partitions={}\n",
    "counter=1\n",
    "print(consumer.assignment())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18485575-6969-4fbd-8646-62477b066fa8",
   "metadata": {},
   "source": [
    "for i in part_nums:\n",
    "    partitions['partition']=load_partition(i)['partition'] # key=partition num, value=snapshot dict\n",
    "    if 'offset' in partitions.keys():\n",
    "        print('loading previous offset')\n",
    "        print('PARTITION: ',i)\n",
    "        consumer.seek(TopicPartition('stations',i),partitions['offset'])\n",
    "    else:\n",
    "        print('NEW READ , ',i)\n",
    "        consumer.seek_to_beginning(TopicPartition('stations',i)) # else\n",
    "    batch = consumer.poll(1000)\n",
    "    date_list=[]\n",
    "    for tp, messages in batch.items():\n",
    "        print(tp)\n",
    "        for msg in messages:\n",
    "            s = Report.FromString(msg.value)\n",
    "            date_list.append(s.date)\n",
    "            #print(s.station)\n",
    "            if s.station in partitions.keys():\n",
    "                if s.date<=partitions[s.station]['end']:\n",
    "                    break\n",
    "                counter+=1\n",
    "                partitions[s.station]['sum']+=s.degrees\n",
    "                partitions[s.station]['count']+=1\n",
    "                partitions[s.station]['avg']=(partitions[s.station]['sum']/counter)\n",
    "                partitions[s.station]['start'] = s.date if s.date<partitions[s.station]['start'] else partitions[s.station]['start']\n",
    "                partitions[s.station]['end']=s.date\n",
    "            else:\n",
    "                partitions[s.station]={}\n",
    "                partitions[s.station]['sum']=s.degrees\n",
    "                partitions[s.station]['count']=counter\n",
    "                partitions[s.station]['avg']=0\n",
    "                partitions[s.station]['start']=s.date\n",
    "                partitions[s.station]['end']=s.date\n",
    "\n",
    "        partitions['offset']=consumer.position(tp)\n",
    "        save_partition(partitions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
