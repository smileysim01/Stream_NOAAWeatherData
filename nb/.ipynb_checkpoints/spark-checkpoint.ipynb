{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "633b0732-969a-4967-82a5-eaca99fb212e",
   "metadata": {},
   "source": [
    "# simran4@wisc.edu, rgundavarapu@wisc.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13741603-a28a-4553-a190-f94d2be394e2",
   "metadata": {},
   "source": [
    "# Part 3: Spark Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92c4a525-3c92-4df3-a6bf-8d3eb5b6a67f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.10/dist-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-5e0d59bc-6029-4806-ba64-3c1f7ea88316;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.2.2 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.2.2 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.8.1 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.4 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.1 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.1 in central\n",
      "\tfound org.apache.htrace#htrace-core4;4.1.0-incubating in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      ":: resolution report :: resolve 1132ms :: artifacts dl 36ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.1 from central in [default]\n",
      "\torg.apache.htrace#htrace-core4;4.1.0-incubating from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.8.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.2.2 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.2.2 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.4 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   13  |   0   |   0   |   0   ||   13  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-5e0d59bc-6029-4806-ba64-3c1f7ea88316\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 13 already retrieved (0kB/33ms)\n",
      "23/04/30 03:36:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = (SparkSession.builder.appName(\"cs544\")\n",
    "         .config(\"spark.sql.shuffle.partitions\", 10)\n",
    "         .config(\"spark.ui.showConsoleProgress\", False)\n",
    "         .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.2')\n",
    "         .getOrCreate())\n",
    "\n",
    "df = (\n",
    "    spark.readStream.format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\")\n",
    "    .option(\"subscribe\", \"stations-json\")\n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de5a102-bc22-42c4-9982-9ecaf93ea31c",
   "metadata": {},
   "source": [
    "Use a spark.readStream to load the stations-json stream to a DataFrame. Tips:\n",
    "\n",
    "the value comments will contain the bytes you wrote. You can use col(????).cast(\"string\") to convert bytes to a string (this assumes UTF-8 encoding)\n",
    "you can convert a JSON string to a structure using from_json(????, schema). The schema needs to specify the types. It will be something like schema = \"station STRING, date DATE, ...\" for you.\n",
    "if a column named \"value\" is a struct, you can access an entry named \"station\" inside with \"value.station\"\n",
    "Requirements\n",
    "\n",
    "use .option(\"startingOffsets\",\"earliest\") to begin with the earliest data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35e687ad-e747-43ec-978d-e910652b7596",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7782d3ea-3b16-43f9-83a3-aef4f2c95afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/30 03:36:49 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-c3a9ad70-2bed-4561-8df1-2d3e75be3b42. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/04/30 03:36:49 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+------------+------------------+----------+\n",
      "|station|     start|       end|measurements|               avg|       max|\n",
      "+-------+----------+----------+------------+------------------+----------+\n",
      "|      A|2000-01-01|2012-12-19|        4737| 70.80070228447812| 126.68327|\n",
      "|      B|2000-01-01|2012-12-19|        4737| 47.84972212219983|  95.94885|\n",
      "|      C|2000-01-01|2012-12-19|        4737|50.890219023396554| 102.45945|\n",
      "|      D|2000-01-01|2012-12-19|        4737| 58.53811317454518| 111.26711|\n",
      "|      E|2000-01-01|2012-12-19|        4737|  63.4312422058616| 115.00308|\n",
      "|      F|2000-01-01|2012-12-19|        4737| 58.49929810269915|110.423775|\n",
      "|      G|2000-01-01|2012-12-19|        4737|58.025650721198275| 108.84774|\n",
      "|      H|2000-01-01|2012-12-19|        4737| 70.92236849525344| 123.88833|\n",
      "|      I|2000-01-01|2012-12-19|        4737|43.054414635539295| 94.069176|\n",
      "|      J|2000-01-01|2012-12-19|        4737| 66.63101425617839|117.268295|\n",
      "|      K|2000-01-01|2012-12-19|        4737| 68.48960101707641| 116.88782|\n",
      "|      L|2000-01-01|2012-12-19|        4737| 62.88509046394714| 117.50477|\n",
      "|      M|2000-01-01|2012-12-19|        4737| 63.55655029490952|113.735466|\n",
      "|      N|2000-01-01|2012-12-19|        4737|56.515403034177695| 102.48066|\n",
      "|      O|2000-01-01|2012-12-19|        4737| 65.53929724652026| 112.89237|\n",
      "+-------+----------+----------+------------+------------------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/30 03:37:07 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 5000 milliseconds, but spent 17568 milliseconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+------------+------------------+----------+\n",
      "|station|     start|       end|measurements|               avg|       max|\n",
      "+-------+----------+----------+------------+------------------+----------+\n",
      "|      A|2000-01-01|2013-01-02|        4751| 70.71409083466558| 126.68327|\n",
      "|      B|2000-01-01|2013-01-02|        4751|47.778137869068104|  95.94885|\n",
      "|      C|2000-01-01|2013-01-02|        4751| 50.82488708395352| 102.45945|\n",
      "|      D|2000-01-01|2013-01-02|        4751|  58.4267311512207| 111.26711|\n",
      "|      E|2000-01-01|2013-01-02|        4751|63.363434903874094| 115.00308|\n",
      "|      F|2000-01-01|2013-01-02|        4751| 58.42297253375855|110.423775|\n",
      "|      G|2000-01-01|2013-01-02|        4751|57.973112874923316| 108.84774|\n",
      "|      H|2000-01-01|2013-01-02|        4751| 70.84443838434254| 123.88833|\n",
      "|      I|2000-01-01|2013-01-02|        4751| 42.98282142332966| 94.069176|\n",
      "|      J|2000-01-01|2013-01-02|        4751| 66.59679966278011|117.268295|\n",
      "|      K|2000-01-01|2013-01-02|        4751| 68.41576724834277| 116.88782|\n",
      "|      L|2000-01-01|2013-01-02|        4751| 62.76346131310516| 117.50477|\n",
      "|      M|2000-01-01|2013-01-02|        4751| 63.52228003064097|113.735466|\n",
      "|      N|2000-01-01|2013-01-02|        4751| 56.44061331819469| 102.48066|\n",
      "|      O|2000-01-01|2013-01-02|        4751|  65.4794138257214| 112.89237|\n",
      "+-------+----------+----------+------------+------------------+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+------------+------------------+----------+\n",
      "|station|     start|       end|measurements|               avg|       max|\n",
      "+-------+----------+----------+------------+------------------+----------+\n",
      "|      A|2000-01-01|2013-01-05|        4754| 70.70042113832531| 126.68327|\n",
      "|      B|2000-01-01|2013-01-05|        4754|47.758079096530736|  95.94885|\n",
      "|      C|2000-01-01|2013-01-05|        4754| 50.81487609135112| 102.45945|\n",
      "|      D|2000-01-01|2013-01-05|        4754| 58.40418684121486| 111.26711|\n",
      "|      E|2000-01-01|2013-01-05|        4754|63.343929787568804| 115.00308|\n",
      "|      F|2000-01-01|2013-01-05|        4754|  58.4056088818589|110.423775|\n",
      "|      G|2000-01-01|2013-01-05|        4754|  57.9625915433862| 108.84774|\n",
      "|      H|2000-01-01|2013-01-05|        4754| 70.82837151066816| 123.88833|\n",
      "|      I|2000-01-01|2013-01-05|        4754| 42.97239411989702| 94.069176|\n",
      "|      J|2000-01-01|2013-01-05|        4754| 66.58189564589748|117.268295|\n",
      "|      K|2000-01-01|2013-01-05|        4754| 68.40260486197562| 116.88782|\n",
      "|      L|2000-01-01|2013-01-05|        4754|62.741923938842696| 117.50477|\n",
      "|      M|2000-01-01|2013-01-05|        4754|63.508637109522965|113.735466|\n",
      "|      N|2000-01-01|2013-01-05|        4754| 56.41692879615484| 102.48066|\n",
      "|      O|2000-01-01|2013-01-05|        4754| 65.46811116388679| 112.89237|\n",
      "+-------+----------+----------+------------+------------------+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+------------+------------------+----------+\n",
      "|station|     start|       end|measurements|               avg|       max|\n",
      "+-------+----------+----------+------------+------------------+----------+\n",
      "|      A|2000-01-01|2013-01-09|        4758| 70.68007557569506| 126.68327|\n",
      "|      B|2000-01-01|2013-01-09|        4758| 47.73294779093339|  95.94885|\n",
      "|      C|2000-01-01|2013-01-09|        4758| 50.79685929539751| 102.45945|\n",
      "|      D|2000-01-01|2013-01-09|        4758| 58.37135752838868| 111.26711|\n",
      "|      E|2000-01-01|2013-01-09|        4758| 63.31663574927892| 115.00308|\n",
      "|      F|2000-01-01|2013-01-09|        4758| 58.37195068805746|110.423775|\n",
      "|      G|2000-01-01|2013-01-09|        4758|57.942266090420325| 108.84774|\n",
      "|      H|2000-01-01|2013-01-09|        4758|  70.8067317837775| 123.88833|\n",
      "|      I|2000-01-01|2013-01-09|        4758|42.949479509513594| 94.069176|\n",
      "|      J|2000-01-01|2013-01-09|        4758|  66.5731969745583|117.268295|\n",
      "|      K|2000-01-01|2013-01-09|        4758| 68.39070994977241| 116.88782|\n",
      "|      L|2000-01-01|2013-01-09|        4758| 62.70450741085198| 117.50477|\n",
      "|      M|2000-01-01|2013-01-09|        4758|63.489481572194876|113.735466|\n",
      "|      N|2000-01-01|2013-01-09|        4758| 56.39184874064434| 102.48066|\n",
      "|      O|2000-01-01|2013-01-09|        4758| 65.45576115594585| 112.89237|\n",
      "+-------+----------+----------+------------+------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions\n",
    "schema = \"station STRING, date DATE, degrees FLOAT, raining INTEGER\"\n",
    "data = (df.select(col(\"key\").cast(\"string\"),\n",
    "          from_json(col(\"value\").cast(\"string\"), schema).alias(\"value\")))\n",
    "\n",
    "counts_df = data.groupBy('value.station').agg(functions.min('value.date').alias('start'),\n",
    "                                              functions.max('value.date').alias('end'),\n",
    "                                              functions.count('value.station').alias('measurements'),\n",
    "                                              functions.avg('value.degrees').alias('avg'),\n",
    "                                              functions. max('value.degrees').alias('max')).sort('value.station')\n",
    "                 \n",
    "s = counts_df.writeStream.format(\"console\").trigger(processingTime=\"5 seconds\").outputMode(\"complete\").start()\n",
    "s.awaitTermination(30)\n",
    "s.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b68be55-5e05-47b3-a768-32d96b543860",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[station: string, date: date, raining: int]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "today=data.select(col('value.station'),col('value.date'),col('value.raining'))\n",
    "today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "109aea78-5ada-4d38-ba94-d5ed3ee0017f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import month,date_add\n",
    "features_yesterday=data.select(col('value.raining').alias('sub1raining'),col('value.degrees').alias('sub1degrees'),col('value.station'),month(col('value.date')).alias('month'),date_add('value.date', 1).alias('date'))\n",
    "features_2days=data.select(col('value.raining').alias('sub2raining'),col('value.degrees').alias('sub2degrees'),col('value.station'),month(col('value.date')).alias('month'),date_add('value.date', 2).alias('date'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4226de33-d2b4-4f48-821a-aef9bb24ec5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[date: date, month: int, station: string, sub2raining: int, sub2degrees: float, sub1raining: int, sub1degrees: float]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features=features_2days.join(features_yesterday,['date','month','station'],\"inner\") \n",
    "#features_2days.station ==  features_yesterday.station,features_2days.date ==  features_yesterday.date\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3e9c4ed-2b55-48f4-bd5a-4a6f3b9b2487",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[date: date, station: string, raining: int, month: int, sub2raining: int, sub2degrees: float, sub1raining: int, sub1degrees: float]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final= today.join(features,['date','station'],'inner')\n",
    "#today.date==features.date,today.station==features.station\n",
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3f01e9f-db42-409d-b922-59a662dea51e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/30 03:37:20 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "s3 = final.repartition(1).writeStream.format(\"parquet\").option(\"path\", \"./today_features\").option(\"checkpointLocation\",\"./checkpoint/directory\").trigger(processingTime=\"1 minute\").start()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cc6902-05f4-41c9-90fa-7fbed59c03be",
   "metadata": {},
   "source": [
    "# Part 4: Spark ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c9a4777-23c9-43c2-9364-5159874c2480",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6cc3750f-cec4-4681-8dcf-e784bf08defd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[station: string, date: date, raining: int]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2a1e6ca-5a96-4f9a-b5c5-89a653c48d83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[date: date, month: int, station: string, sub2raining: int, sub2degrees: float, sub1raining: int, sub1degrees: float]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ad4f52bc-9a9a-406e-bc96-88ae5f8524cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[date: date, station: string, raining: int, month: int, sub2raining: int, sub2degrees: float, sub1raining: int, sub1degrees: float]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meow=spark.read.parquet(\"./today_features\") \n",
    "meow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "02ade8d3-e201-4794-b3d7-1e12ad840898",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "va = VectorAssembler(inputCols=[\"month\", \"sub1degrees\", \"sub1raining\", \"sub2degrees\", \"sub2raining\"], outputCol=\"features\")\n",
    "train_data, test_data = meow.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "04068fa4-8dc1-45ee-8dbf-c491658d4ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/30 03:38:11 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "23/04/30 03:38:11 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "23/04/30 03:38:12 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "23/04/30 03:38:12 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "23/04/30 03:38:12 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "23/04/30 03:38:12 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "23/04/30 03:38:12 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "23/04/30 03:38:12 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "23/04/30 03:38:12 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "23/04/30 03:38:12 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "23/04/30 03:38:12 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "23/04/30 03:38:12 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "23/04/30 03:38:12 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "23/04/30 03:38:12 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "23/04/30 03:38:12 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "23/04/30 03:38:12 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "23/04/30 03:38:13 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "23/04/30 03:38:13 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "23/04/30 03:38:13 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "23/04/30 03:38:13 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "23/04/30 03:38:13 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "23/04/30 03:38:13 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "23/04/30 03:38:13 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "23/04/30 03:38:13 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "23/04/30 03:38:14 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "23/04/30 03:38:14 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "23/04/30 03:38:14 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "23/04/30 03:38:14 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "23/04/30 03:38:14 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "23/04/30 03:38:14 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "23/04/30 03:38:14 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "23/04/30 03:38:14 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "23/04/30 03:38:15 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "23/04/30 03:38:15 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "23/04/30 03:38:15 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "23/04/30 03:38:15 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "23/04/30 03:38:15 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "23/04/30 03:38:15 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "23/04/30 03:38:15 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "23/04/30 03:38:15 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "23/04/30 03:38:15 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "23/04/30 03:38:15 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "23/04/30 03:38:15 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "23/04/30 03:38:15 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "23/04/30 03:38:16 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "23/04/30 03:38:16 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "23/04/30 03:38:16 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "23/04/30 03:38:16 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "23/04/30 03:38:16 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "23/04/30 03:38:17 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "23/04/30 03:38:17 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "23/04/30 03:38:17 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "23/04/30 03:38:17 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "23/04/30 03:38:17 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "23/04/30 03:38:17 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "23/04/30 03:38:17 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "23/04/30 03:38:17 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "23/04/30 03:38:17 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "23/04/30 03:38:17 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "23/04/30 03:38:17 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "23/04/30 03:38:17 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "23/04/30 03:38:17 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "23/04/30 03:38:18 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "23/04/30 03:38:18 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "23/04/30 03:38:18 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "23/04/30 03:38:18 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "23/04/30 03:38:18 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "23/04/30 03:38:18 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "23/04/30 03:38:18 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "23/04/30 03:38:18 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "23/04/30 03:38:18 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "23/04/30 03:38:18 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "23/04/30 03:38:18 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "23/04/30 03:38:18 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "23/04/30 03:38:18 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "23/04/30 03:38:18 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "23/04/30 03:38:18 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "23/04/30 03:38:18 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "23/04/30 03:38:18 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "23/04/30 03:38:19 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "23/04/30 03:38:21 WARN CheckpointFileManager: Failed to rename temp file today_features/_spark_metadata/.1.df20f00d-0d8f-46f1-a36f-50cb92b0fd96.tmp to today_features/_spark_metadata/1 because file exists\n",
      "org.apache.hadoop.fs.FileAlreadyExistsException: Rename destination file:/notebooks/today_features/_spark_metadata/.1.crc already exists.\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:770)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:346)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat org.apache.spark.sql.execution.streaming.HDFSMetadataLog.$anonfun$addNewBatchByStream$2(HDFSMetadataLog.scala:176)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.HDFSMetadataLog.addNewBatchByStream(HDFSMetadataLog.scala:171)\n",
      "\tat org.apache.spark.sql.execution.streaming.HDFSMetadataLog.add(HDFSMetadataLog.scala:116)\n",
      "\tat org.apache.spark.sql.execution.streaming.CompactibleFileStreamLog.add(CompactibleFileStreamLog.scala:168)\n",
      "\tat org.apache.spark.sql.execution.streaming.ManifestFileCommitProtocol.commitJob(ManifestFileCommitProtocol.scala:76)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$20(FileFormatWriter.scala:240)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:634)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:240)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileStreamSink.addBatch(FileStreamSink.scala:181)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:600)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:598)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:598)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:228)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:193)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:57)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:187)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:303)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:286)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:209)\n",
      "23/04/30 03:38:21 ERROR FileFormatWriter: Aborting job cb9d3b64-66b6-4a5a-b0f0-b363e3ba4831.\n",
      "java.util.ConcurrentModificationException: Multiple streaming queries are concurrently using today_features/_spark_metadata\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.multiStreamingQueriesUsingPathConcurrentlyError(QueryExecutionErrors.scala:1420)\n",
      "\tat org.apache.spark.sql.execution.streaming.HDFSMetadataLog.$anonfun$addNewBatchByStream$2(HDFSMetadataLog.scala:182)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.HDFSMetadataLog.addNewBatchByStream(HDFSMetadataLog.scala:171)\n",
      "\tat org.apache.spark.sql.execution.streaming.HDFSMetadataLog.add(HDFSMetadataLog.scala:116)\n",
      "\tat org.apache.spark.sql.execution.streaming.CompactibleFileStreamLog.add(CompactibleFileStreamLog.scala:168)\n",
      "\tat org.apache.spark.sql.execution.streaming.ManifestFileCommitProtocol.commitJob(ManifestFileCommitProtocol.scala:76)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$20(FileFormatWriter.scala:240)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:634)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:240)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileStreamSink.addBatch(FileStreamSink.scala:181)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:600)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:598)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:598)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:228)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:193)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:57)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:187)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:303)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:286)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:209)\n",
      "Caused by: org.apache.hadoop.fs.FileAlreadyExistsException: Rename destination file:/notebooks/today_features/_spark_metadata/.1.crc already exists.\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:770)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:346)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat org.apache.spark.sql.execution.streaming.HDFSMetadataLog.$anonfun$addNewBatchByStream$2(HDFSMetadataLog.scala:176)\n",
      "\t... 35 more\n",
      "23/04/30 03:38:22 ERROR MicroBatchExecution: Query [id = bdfbc3b3-a886-469c-b347-182c8f056deb, runId = 4682f61b-a625-4e5b-9971-f9163b857e8d] terminated with error\n",
      "org.apache.spark.SparkException: Job aborted.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:251)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileStreamSink.addBatch(FileStreamSink.scala:181)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:600)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:598)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:598)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:228)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:193)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:57)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:187)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:303)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:286)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:209)\n",
      "Caused by: java.util.ConcurrentModificationException: Multiple streaming queries are concurrently using today_features/_spark_metadata\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.multiStreamingQueriesUsingPathConcurrentlyError(QueryExecutionErrors.scala:1420)\n",
      "\tat org.apache.spark.sql.execution.streaming.HDFSMetadataLog.$anonfun$addNewBatchByStream$2(HDFSMetadataLog.scala:182)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.HDFSMetadataLog.addNewBatchByStream(HDFSMetadataLog.scala:171)\n",
      "\tat org.apache.spark.sql.execution.streaming.HDFSMetadataLog.add(HDFSMetadataLog.scala:116)\n",
      "\tat org.apache.spark.sql.execution.streaming.CompactibleFileStreamLog.add(CompactibleFileStreamLog.scala:168)\n",
      "\tat org.apache.spark.sql.execution.streaming.ManifestFileCommitProtocol.commitJob(ManifestFileCommitProtocol.scala:76)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$20(FileFormatWriter.scala:240)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:634)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:240)\n",
      "\t... 25 more\n",
      "Caused by: org.apache.hadoop.fs.FileAlreadyExistsException: Rename destination file:/notebooks/today_features/_spark_metadata/.1.crc already exists.\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:770)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:346)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat org.apache.spark.sql.execution.streaming.HDFSMetadataLog.$anonfun$addNewBatchByStream$2(HDFSMetadataLog.scala:176)\n",
      "\t... 35 more\n"
     ]
    }
   ],
   "source": [
    "dt_classifier = DecisionTreeClassifier(featuresCol='features',labelCol='raining')\n",
    "dt_model = dt_classifier.fit(va.transform(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f921caa1-c386-4414-954b-c9920e66d35a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassificationModel: uid=DecisionTreeClassifier_72f5307bfc1a, depth=5, numNodes=11, numClasses=2, numFeatures=5\n",
      "  If (feature 2 <= 0.5)\n",
      "   Predict: 0.0\n",
      "  Else (feature 2 > 0.5)\n",
      "   If (feature 1 <= 39.81497764587402)\n",
      "    If (feature 4 <= 0.5)\n",
      "     Predict: 0.0\n",
      "    Else (feature 4 > 0.5)\n",
      "     If (feature 1 <= 37.14095497131348)\n",
      "      If (feature 0 <= 2.5)\n",
      "       Predict: 0.0\n",
      "      Else (feature 0 > 2.5)\n",
      "       Predict: 1.0\n",
      "     Else (feature 1 > 37.14095497131348)\n",
      "      Predict: 1.0\n",
      "   Else (feature 1 > 39.81497764587402)\n",
      "    Predict: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dt_model.toDebugString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f11c5cbb-55a8-4df4-acd0-9fc46c50e93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg(correct): 0.7876100271444441\n",
      "+------------------+\n",
      "|      avg(raining)|\n",
      "+------------------+\n",
      "|0.3502347417840376|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = dt_model.transform(va.transform(test_data))\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol='prediction',labelCol='raining')\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"avg(correct):\",accuracy)\n",
    "meow.agg(functions.avg('raining').alias('avg(raining)')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "57421e68-f82f-421f-be64-280b449cdfcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[date: date, month: int, station: string, sub2raining: int, sub2degrees: float, sub1raining: int, sub1degrees: float]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a80e294b-f7d5-4d2f-91d5-c68c11cf01da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/30 03:59:07 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-f0c5addb-fc9a-4e49-b965-12ab576545e9. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/04/30 03:59:07 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+\n",
      "|station|      date|prediction|\n",
      "+-------+----------+----------+\n",
      "|      A|2000-01-06|       0.0|\n",
      "|      A|2000-01-10|       0.0|\n",
      "|      A|2000-01-16|       0.0|\n",
      "|      A|2000-01-24|       0.0|\n",
      "|      A|2000-01-26|       0.0|\n",
      "|      A|2000-02-10|       0.0|\n",
      "|      A|2000-02-11|       0.0|\n",
      "|      A|2000-02-17|       0.0|\n",
      "|      A|2000-02-18|       0.0|\n",
      "|      A|2000-02-20|       1.0|\n",
      "|      A|2000-03-07|       0.0|\n",
      "|      A|2000-03-21|       0.0|\n",
      "|      A|2000-04-21|       1.0|\n",
      "|      A|2000-04-29|       0.0|\n",
      "|      A|2000-05-08|       1.0|\n",
      "|      A|2000-05-18|       0.0|\n",
      "|      A|2000-06-01|       0.0|\n",
      "|      A|2000-06-05|       0.0|\n",
      "|      A|2000-06-06|       1.0|\n",
      "|      A|2000-06-11|       1.0|\n",
      "+-------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/30 03:59:21 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000 milliseconds, but spent 13923 milliseconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+\n",
      "|station|      date|prediction|\n",
      "+-------+----------+----------+\n",
      "|      A|2016-08-23|       0.0|\n",
      "|      A|2016-08-13|       0.0|\n",
      "|      A|2016-08-19|       0.0|\n",
      "|      A|2016-08-20|       0.0|\n",
      "|      A|2016-08-21|       0.0|\n",
      "|      A|2016-08-14|       0.0|\n",
      "|      A|2016-08-16|       0.0|\n",
      "|      A|2016-08-17|       0.0|\n",
      "|      A|2016-08-15|       0.0|\n",
      "|      A|2016-08-18|       0.0|\n",
      "|      A|2016-08-22|       0.0|\n",
      "+-------+----------+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+\n",
      "|station|      date|prediction|\n",
      "+-------+----------+----------+\n",
      "|      A|2016-08-25|       0.0|\n",
      "|      A|2016-08-31|       0.0|\n",
      "|      A|2016-08-27|       0.0|\n",
      "|      A|2016-08-30|       0.0|\n",
      "|      A|2016-08-24|       0.0|\n",
      "|      A|2016-08-29|       0.0|\n",
      "|      A|2016-08-28|       0.0|\n",
      "|      A|2016-08-26|       0.0|\n",
      "+-------+----------+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+\n",
      "|station|      date|prediction|\n",
      "+-------+----------+----------+\n",
      "|      A|2016-09-05|       0.0|\n",
      "|      A|2016-09-04|       0.0|\n",
      "|      A|2016-09-06|       1.0|\n",
      "|      A|2016-09-09|       1.0|\n",
      "|      A|2016-09-10|       1.0|\n",
      "|      A|2016-09-01|       0.0|\n",
      "|      A|2016-09-03|       0.0|\n",
      "|      A|2016-09-08|       1.0|\n",
      "|      A|2016-09-07|       0.0|\n",
      "+-------+----------+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 4\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+\n",
      "|station|      date|prediction|\n",
      "+-------+----------+----------+\n",
      "|      A|2016-09-16|       0.0|\n",
      "|      A|2016-09-17|       0.0|\n",
      "|      A|2016-09-18|       1.0|\n",
      "|      A|2016-09-12|       0.0|\n",
      "|      A|2016-09-13|       0.0|\n",
      "|      A|2016-09-14|       0.0|\n",
      "|      A|2016-09-20|       0.0|\n",
      "|      A|2016-09-11|       0.0|\n",
      "|      A|2016-09-15|       0.0|\n",
      "|      A|2016-09-19|       0.0|\n",
      "+-------+----------+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 5\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+\n",
      "|station|      date|prediction|\n",
      "+-------+----------+----------+\n",
      "|      A|2016-09-30|       0.0|\n",
      "|      A|2016-09-23|       0.0|\n",
      "|      A|2016-09-29|       0.0|\n",
      "|      A|2016-09-21|       0.0|\n",
      "|      A|2016-09-24|       1.0|\n",
      "|      A|2016-09-28|       1.0|\n",
      "|      A|2016-09-25|       1.0|\n",
      "|      A|2016-09-26|       0.0|\n",
      "|      A|2016-09-27|       0.0|\n",
      "|      A|2016-09-22|       0.0|\n",
      "+-------+----------+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 6\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+\n",
      "|station|      date|prediction|\n",
      "+-------+----------+----------+\n",
      "|      A|2016-10-05|       0.0|\n",
      "|      A|2016-10-04|       0.0|\n",
      "|      A|2016-10-09|       0.0|\n",
      "|      A|2016-10-01|       0.0|\n",
      "|      A|2016-10-08|       1.0|\n",
      "|      A|2016-10-06|       0.0|\n",
      "|      A|2016-10-03|       1.0|\n",
      "|      A|2016-10-07|       1.0|\n",
      "|      A|2016-10-10|       1.0|\n",
      "+-------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "p_model = dt_model.transform(va.transform(features)).select(\"station\", \"date\", \"prediction\").filter(\"station='A'\")\n",
    "q2 = (p_model.writeStream.format(\"console\")\n",
    ".trigger(processingTime=\"10 seconds\").start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7d38bbb6-cbb1-423d-8b99-bdff14c1a18d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/30 04:00:21 ERROR WriteToDataSourceV2Exec: Data source write support org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@8909a88 is aborting.\n",
      "23/04/30 04:00:21 ERROR WriteToDataSourceV2Exec: Data source write support org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@8909a88 aborted.\n",
      "23/04/30 04:00:22 ERROR DiskBlockObjectWriter: Exception occurred while reverting partial writes to file /tmp/blockmgr-fb733666-d7f5-436e-9e42-97d45efdbb1b/21/temp_shuffle_36abaf89-e826-430b-bf16-72437582bbab, null\n",
      "23/04/30 04:00:22 ERROR DiskBlockObjectWriter: Exception occurred while reverting partial writes to file /tmp/blockmgr-fb733666-d7f5-436e-9e42-97d45efdbb1b/18/temp_shuffle_1cf66869-9037-4cf3-8fea-156acb4319e9, null\n",
      "23/04/30 04:00:22 ERROR DiskBlockObjectWriter: Exception occurred while reverting partial writes to file /tmp/blockmgr-fb733666-d7f5-436e-9e42-97d45efdbb1b/36/temp_shuffle_21ae1745-161c-42ec-afb0-ae267ca8c5ee, null\n",
      "23/04/30 04:00:22 ERROR DiskBlockObjectWriter: Exception occurred while reverting partial writes to file /tmp/blockmgr-fb733666-d7f5-436e-9e42-97d45efdbb1b/2b/temp_shuffle_45841a0f-4926-4e47-ae0f-3307a49aefcf, null\n",
      "23/04/30 04:00:22 ERROR DiskBlockObjectWriter: Exception occurred while reverting partial writes to file /tmp/blockmgr-fb733666-d7f5-436e-9e42-97d45efdbb1b/3c/temp_shuffle_00b854a1-cb33-41b8-8069-3a0db41df1fa, null\n",
      "23/04/30 04:00:22 ERROR DiskBlockObjectWriter: Exception occurred while reverting partial writes to file /tmp/blockmgr-fb733666-d7f5-436e-9e42-97d45efdbb1b/05/temp_shuffle_2adb5973-a38a-42eb-806c-9dc35bb9d1dd, null\n",
      "23/04/30 04:00:22 ERROR DiskBlockObjectWriter: Exception occurred while reverting partial writes to file /tmp/blockmgr-fb733666-d7f5-436e-9e42-97d45efdbb1b/35/temp_shuffle_66661c7a-6369-472b-b852-1294c5abb869, null\n",
      "23/04/30 04:00:22 ERROR DiskBlockObjectWriter: Exception occurred while reverting partial writes to file /tmp/blockmgr-fb733666-d7f5-436e-9e42-97d45efdbb1b/3e/temp_shuffle_bf701afd-3d6c-4ee6-a02f-a3da9f74203a, null\n",
      "23/04/30 04:00:22 ERROR DiskBlockObjectWriter: Exception occurred while reverting partial writes to file /tmp/blockmgr-fb733666-d7f5-436e-9e42-97d45efdbb1b/2a/temp_shuffle_4a99aa0b-4459-462b-b4e8-57e18e646053, null\n",
      "23/04/30 04:00:22 ERROR DiskBlockObjectWriter: Exception occurred while reverting partial writes to file /tmp/blockmgr-fb733666-d7f5-436e-9e42-97d45efdbb1b/14/temp_shuffle_9a9facb2-310b-424d-960f-7a44d37abb28, null\n",
      "23/04/30 04:00:22 ERROR DiskBlockObjectWriter: Exception occurred while reverting partial writes to file /tmp/blockmgr-fb733666-d7f5-436e-9e42-97d45efdbb1b/1d/temp_shuffle_35a64c38-f24e-49aa-aef5-af268e365075, null\n",
      "23/04/30 04:00:22 ERROR DiskBlockObjectWriter: Exception occurred while reverting partial writes to file /tmp/blockmgr-fb733666-d7f5-436e-9e42-97d45efdbb1b/32/temp_shuffle_fdbb6364-a857-4fcd-b165-0b4bdea5738c, null\n",
      "23/04/30 04:00:22 ERROR DiskBlockObjectWriter: Exception occurred while reverting partial writes to file /tmp/blockmgr-fb733666-d7f5-436e-9e42-97d45efdbb1b/12/temp_shuffle_796c591a-ceb7-4326-9f06-8d03fbb71da3, null\n",
      "23/04/30 04:00:22 ERROR DiskBlockObjectWriter: Exception occurred while reverting partial writes to file /tmp/blockmgr-fb733666-d7f5-436e-9e42-97d45efdbb1b/3a/temp_shuffle_9e123a1b-c63c-494d-9f19-900f481e9b20, null\n",
      "23/04/30 04:00:22 ERROR DiskBlockObjectWriter: Exception occurred while reverting partial writes to file /tmp/blockmgr-fb733666-d7f5-436e-9e42-97d45efdbb1b/3a/temp_shuffle_9eae4742-b13f-45dc-ab0f-76b7d2193e1f, null\n",
      "23/04/30 04:00:22 ERROR DiskBlockObjectWriter: Exception occurred while reverting partial writes to file /tmp/blockmgr-fb733666-d7f5-436e-9e42-97d45efdbb1b/01/temp_shuffle_f49b59dc-5247-4059-9a28-d50ab886b592, null\n",
      "23/04/30 04:00:22 ERROR DiskBlockObjectWriter: Exception occurred while reverting partial writes to file /tmp/blockmgr-fb733666-d7f5-436e-9e42-97d45efdbb1b/3d/temp_shuffle_49a04fa5-debb-43b8-a043-275d7c7f7fb0, null\n",
      "23/04/30 04:00:22 ERROR DiskBlockObjectWriter: Exception occurred while reverting partial writes to file /tmp/blockmgr-fb733666-d7f5-436e-9e42-97d45efdbb1b/37/temp_shuffle_09cc8eb5-2ec9-46dc-9066-c489f8617653, null\n",
      "23/04/30 04:00:22 ERROR DiskBlockObjectWriter: Exception occurred while reverting partial writes to file /tmp/blockmgr-fb733666-d7f5-436e-9e42-97d45efdbb1b/34/temp_shuffle_c2cb825b-534c-4fd7-920b-19cb5ab7665f, null\n",
      "23/04/30 04:00:22 ERROR DiskBlockObjectWriter: Exception occurred while reverting partial writes to file /tmp/blockmgr-fb733666-d7f5-436e-9e42-97d45efdbb1b/3e/temp_shuffle_5a00d743-3b9c-4fe8-bce2-17efbf148afd, null\n",
      "23/04/30 04:00:22 WARN TaskSetManager: Lost task 0.0 in stage 89.0 (TID 380) (9a9fd8f47412 executor driver): TaskKilled (Stage cancelled)\n",
      "23/04/30 04:00:22 WARN TaskSetManager: Lost task 1.0 in stage 89.0 (TID 381) (9a9fd8f47412 executor driver): TaskKilled (Stage cancelled)\n"
     ]
    }
   ],
   "source": [
    "q2.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18acb9de-e5d1-4473-9e37-4d9c9e46cf88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
