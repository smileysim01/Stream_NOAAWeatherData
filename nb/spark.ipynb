{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "633b0732-969a-4967-82a5-eaca99fb212e",
   "metadata": {},
   "source": [
    "# simran4@wisc.edu, rgundavarapu@wisc.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13741603-a28a-4553-a190-f94d2be394e2",
   "metadata": {},
   "source": [
    "# Part 3: Spark Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92c4a525-3c92-4df3-a6bf-8d3eb5b6a67f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.10/dist-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-22eb1df4-4d36-41a6-9a9f-1267d7eeb077;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.2.2 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.2.2 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.8.1 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.4 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.1 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.1 in central\n",
      "\tfound org.apache.htrace#htrace-core4;4.1.0-incubating in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      ":: resolution report :: resolve 1202ms :: artifacts dl 27ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.1 from central in [default]\n",
      "\torg.apache.htrace#htrace-core4;4.1.0-incubating from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.8.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.2.2 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.2.2 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.4 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   13  |   0   |   0   |   0   ||   13  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-22eb1df4-4d36-41a6-9a9f-1267d7eeb077\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 13 already retrieved (0kB/16ms)\n",
      "23/04/30 02:25:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = (SparkSession.builder.appName(\"cs544\")\n",
    "         .config(\"spark.sql.shuffle.partitions\", 10)\n",
    "         .config(\"spark.ui.showConsoleProgress\", False)\n",
    "         .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.2')\n",
    "         .getOrCreate())\n",
    "\n",
    "df = (\n",
    "    spark.readStream.format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\")\n",
    "    .option(\"subscribe\", \"stations-json\")\n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de5a102-bc22-42c4-9982-9ecaf93ea31c",
   "metadata": {},
   "source": [
    "Use a spark.readStream to load the stations-json stream to a DataFrame. Tips:\n",
    "\n",
    "the value comments will contain the bytes you wrote. You can use col(????).cast(\"string\") to convert bytes to a string (this assumes UTF-8 encoding)\n",
    "you can convert a JSON string to a structure using from_json(????, schema). The schema needs to specify the types. It will be something like schema = \"station STRING, date DATE, ...\" for you.\n",
    "if a column named \"value\" is a struct, you can access an entry named \"station\" inside with \"value.station\"\n",
    "Requirements\n",
    "\n",
    "use .option(\"startingOffsets\",\"earliest\") to begin with the earliest data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35e687ad-e747-43ec-978d-e910652b7596",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7782d3ea-3b16-43f9-83a3-aef4f2c95afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/30 02:26:01 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-f49ec4f8-8c11-413f-ac99-da22e5a0bfb5. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/04/30 02:26:01 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+------------+------------------+----------+\n",
      "|station|     start|       end|measurements|               avg|       max|\n",
      "+-------+----------+----------+------------+------------------+----------+\n",
      "|      A|2000-01-01|2001-05-22|         508| 65.90495668246051|107.278824|\n",
      "|      B|2000-01-01|2001-05-22|         508| 43.17990932983207| 92.984245|\n",
      "|      C|2000-01-01|2001-05-22|         508| 48.03097364083519| 98.619316|\n",
      "|      D|2000-01-01|2001-05-22|         508|53.842927794756854| 103.26719|\n",
      "|      E|2000-01-01|2001-05-22|         508| 61.21108461740449|111.860794|\n",
      "|      F|2000-01-01|2001-05-22|         508|  53.8868232824671|110.423775|\n",
      "|      G|2000-01-01|2001-05-22|         508| 53.72359361423282|100.010826|\n",
      "|      H|2000-01-01|2001-05-22|         508| 66.30751238470002| 116.85257|\n",
      "|      I|2000-01-01|2001-05-22|         508| 37.64841536461838|  81.53725|\n",
      "|      J|2000-01-01|2001-05-22|         508| 61.95553877973181|109.632904|\n",
      "|      K|2000-01-01|2001-05-22|         508| 65.91115464187982| 112.85631|\n",
      "|      L|2000-01-01|2001-05-22|         508|60.258706107852966|114.348595|\n",
      "|      M|2000-01-01|2001-05-22|         508| 59.50114149559201| 101.93959|\n",
      "|      N|2000-01-01|2001-05-22|         508| 51.96824711795867| 100.88238|\n",
      "|      O|2000-01-01|2001-05-22|         508| 62.02758353526198|112.041855|\n",
      "+-------+----------+----------+------------+------------------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/30 02:26:17 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 5000 milliseconds, but spent 15658 milliseconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+------------+------------------+----------+\n",
      "|station|     start|       end|measurements|               avg|       max|\n",
      "+-------+----------+----------+------------+------------------+----------+\n",
      "|      A|2000-01-01|2001-06-02|         519| 66.15168322671585|107.278824|\n",
      "|      B|2000-01-01|2001-06-02|         519|  43.4042908544715| 92.984245|\n",
      "|      C|2000-01-01|2001-06-02|         519| 48.15941479107318| 98.619316|\n",
      "|      D|2000-01-01|2001-06-02|         519| 54.17535851410588| 103.26719|\n",
      "|      E|2000-01-01|2001-06-02|         519| 61.41431935260751|111.860794|\n",
      "|      F|2000-01-01|2001-06-02|         519|54.161726527131364|110.423775|\n",
      "|      G|2000-01-01|2001-06-02|         519| 54.19844453550717|100.010826|\n",
      "|      H|2000-01-01|2001-06-02|         519| 66.78008594770192| 116.85257|\n",
      "|      I|2000-01-01|2001-06-02|         519| 37.91120454387628|  81.53725|\n",
      "|      J|2000-01-01|2001-06-02|         519| 62.16491324364105|109.632904|\n",
      "|      K|2000-01-01|2001-06-02|         519| 66.20179100992132| 112.85631|\n",
      "|      L|2000-01-01|2001-06-02|         519|60.811214888026946|114.348595|\n",
      "|      M|2000-01-01|2001-06-02|         519| 60.00136267748412| 101.93959|\n",
      "|      N|2000-01-01|2001-06-02|         519| 52.17730200313649| 100.88238|\n",
      "|      O|2000-01-01|2001-06-02|         519| 62.35672821750531|112.041855|\n",
      "+-------+----------+----------+------------+------------------+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+------------+------------------+----------+\n",
      "|station|     start|       end|measurements|               avg|       max|\n",
      "+-------+----------+----------+------------+------------------+----------+\n",
      "|      A|2000-01-01|2001-06-06|         523| 66.23950771528722|107.278824|\n",
      "|      B|2000-01-01|2001-06-06|         523| 43.57684787794463| 92.984245|\n",
      "|      C|2000-01-01|2001-06-06|         523|48.301595775278756| 98.619316|\n",
      "|      D|2000-01-01|2001-06-06|         523| 54.33138128995439| 103.26719|\n",
      "|      E|2000-01-01|2001-06-06|         523| 61.47898085395419|111.860794|\n",
      "|      F|2000-01-01|2001-06-06|         523| 54.21485783082805|110.423775|\n",
      "|      G|2000-01-01|2001-06-06|         523| 54.46355918169478|100.010826|\n",
      "|      H|2000-01-01|2001-06-06|         523| 67.11307827236547| 116.85257|\n",
      "|      I|2000-01-01|2001-06-06|         523|38.120979493252406|  81.53725|\n",
      "|      J|2000-01-01|2001-06-06|         523| 62.31006940327457|109.632904|\n",
      "|      K|2000-01-01|2001-06-06|         523| 66.48671998850018| 112.85631|\n",
      "|      L|2000-01-01|2001-06-06|         523| 61.02171247639118|114.348595|\n",
      "|      M|2000-01-01|2001-06-06|         523| 60.20611464316257| 101.93959|\n",
      "|      N|2000-01-01|2001-06-06|         523| 52.34696031665255| 100.88238|\n",
      "|      O|2000-01-01|2001-06-06|         523| 62.55961932734823|112.041855|\n",
      "+-------+----------+----------+------------+------------------+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+------------+------------------+----------+\n",
      "|station|     start|       end|measurements|               avg|       max|\n",
      "+-------+----------+----------+------------+------------------+----------+\n",
      "|      A|2000-01-01|2001-06-09|         526| 66.32317208699854|107.278824|\n",
      "|      B|2000-01-01|2001-06-09|         526| 43.75042422110137| 92.984245|\n",
      "|      C|2000-01-01|2001-06-09|         526|48.460609671620816| 98.619316|\n",
      "|      D|2000-01-01|2001-06-09|         526|54.566262579689464| 103.26719|\n",
      "|      E|2000-01-01|2001-06-09|         526| 61.55731096376484|111.860794|\n",
      "|      F|2000-01-01|2001-06-09|         526| 54.34717406428812|110.423775|\n",
      "|      G|2000-01-01|2001-06-09|         526| 54.63109212955141|100.010826|\n",
      "|      H|2000-01-01|2001-06-09|         526| 67.34175157365691| 116.85257|\n",
      "|      I|2000-01-01|2001-06-09|         526|38.292265193997224|  81.53725|\n",
      "|      J|2000-01-01|2001-06-09|         526|62.425285934042115|109.632904|\n",
      "|      K|2000-01-01|2001-06-09|         526| 66.64516694826771| 112.85631|\n",
      "|      L|2000-01-01|2001-06-09|         526|61.181494136273635|114.348595|\n",
      "|      M|2000-01-01|2001-06-09|         526| 60.37345829662715| 101.93959|\n",
      "|      N|2000-01-01|2001-06-09|         526| 52.46917413214767| 100.88238|\n",
      "|      O|2000-01-01|2001-06-09|         526|62.621111068435496|112.041855|\n",
      "+-------+----------+----------+------------+------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions\n",
    "schema = \"station STRING, date DATE, degrees FLOAT, raining INTEGER\"\n",
    "data = (df.select(col(\"key\").cast(\"string\"),\n",
    "          from_json(col(\"value\").cast(\"string\"), schema).alias(\"value\")))\n",
    "\n",
    "counts_df = data.groupBy('value.station').agg(functions.min('value.date').alias('start'),\n",
    "                                              functions.max('value.date').alias('end'),\n",
    "                                              functions.count('value.station').alias('measurements'),\n",
    "                                              functions.avg('value.degrees').alias('avg'),\n",
    "                                              functions. max('value.degrees').alias('max')).sort('value.station')\n",
    "                 \n",
    "s = counts_df.writeStream.format(\"console\").trigger(processingTime=\"5 seconds\").outputMode(\"complete\").start()\n",
    "s.awaitTermination(30)\n",
    "s.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b68be55-5e05-47b3-a768-32d96b543860",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[station: string, date: date, raining: int]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "today=data.select(col('value.station'),col('value.date'),col('value.raining'))\n",
    "today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "109aea78-5ada-4d38-ba94-d5ed3ee0017f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import month,date_add\n",
    "features_yesterday=data.select(col('value.raining').alias('sub1raining'),col('value.degrees').alias('sub1degrees'),col('value.station'),month(col('value.date')).alias('month'),date_add('value.date', 1).alias('date'))\n",
    "features_2days=data.select(col('value.raining').alias('sub2raining'),col('value.degrees').alias('sub2degrees'),col('value.station'),month(col('value.date')).alias('month'),date_add('value.date', 2).alias('date'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4226de33-d2b4-4f48-821a-aef9bb24ec5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[date: date, month: int, station: string, sub2raining: int, sub2degrees: float, sub1raining: int, sub1degrees: float]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features=features_2days.join(features_yesterday,['date','month','station'],\"inner\") \n",
    "#features_2days.station ==  features_yesterday.station,features_2days.date ==  features_yesterday.date\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3e9c4ed-2b55-48f4-bd5a-4a6f3b9b2487",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[date: date, station: string, raining: int, month: int, sub2raining: int, sub2degrees: float, sub1raining: int, sub1degrees: float]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final= today.join(features,['date','station'],'inner')\n",
    "#today.date==features.date,today.station==features.station\n",
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c3f01e9f-db42-409d-b922-59a662dea51e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/30 02:44:57 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "23/04/30 02:45:00 WARN CheckpointFileManager: Failed to rename temp file file:/notebooks/checkpoint/directory/sources/0/.0.6e4aeac1-2474-49ca-a9a9-6e52d07c7873.tmp to file:/notebooks/checkpoint/directory/sources/0/0 because file exists\n",
      "org.apache.hadoop.fs.FileAlreadyExistsException: Rename destination file:/notebooks/checkpoint/directory/sources/0/.0.crc already exists.\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:770)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:346)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat org.apache.spark.sql.execution.streaming.HDFSMetadataLog.$anonfun$addNewBatchByStream$2(HDFSMetadataLog.scala:176)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.HDFSMetadataLog.addNewBatchByStream(HDFSMetadataLog.scala:171)\n",
      "\tat org.apache.spark.sql.execution.streaming.HDFSMetadataLog.add(HDFSMetadataLog.scala:116)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:251)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:236)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:98)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$5(MicroBatchExecution.scala:394)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$3(MicroBatchExecution.scala:394)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:387)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:384)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:627)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:380)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:210)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:193)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:57)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:187)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:303)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:286)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:209)\n",
      "23/04/30 02:45:00 ERROR MicroBatchExecution: Query [id = d1e3ecca-0cd0-4cc1-b8c4-3a6315f27b59, runId = 5715d852-0264-4a2b-8316-390aad69c846] terminated with error\n",
      "java.util.ConcurrentModificationException: Multiple streaming queries are concurrently using file:/notebooks/checkpoint/directory/sources/0\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.multiStreamingQueriesUsingPathConcurrentlyError(QueryExecutionErrors.scala:1420)\n",
      "\tat org.apache.spark.sql.execution.streaming.HDFSMetadataLog.$anonfun$addNewBatchByStream$2(HDFSMetadataLog.scala:182)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.HDFSMetadataLog.addNewBatchByStream(HDFSMetadataLog.scala:171)\n",
      "\tat org.apache.spark.sql.execution.streaming.HDFSMetadataLog.add(HDFSMetadataLog.scala:116)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:251)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:236)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:98)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$5(MicroBatchExecution.scala:394)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$3(MicroBatchExecution.scala:394)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:387)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:384)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:627)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:380)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:210)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:193)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:57)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:187)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:303)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:286)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:209)\n",
      "Caused by: org.apache.hadoop.fs.FileAlreadyExistsException: Rename destination file:/notebooks/checkpoint/directory/sources/0/.0.crc already exists.\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:770)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:346)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat org.apache.spark.sql.execution.streaming.HDFSMetadataLog.$anonfun$addNewBatchByStream$2(HDFSMetadataLog.scala:176)\n",
      "\t... 42 more\n"
     ]
    }
   ],
   "source": [
    "s3 = final.repartition(1).writeStream.format(\"parquet\").option(\"path\", \"./today_features\").option(\"checkpointLocation\",\"./checkpoint/directory\").trigger(processingTime=\"1 minute\").start()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cc6902-05f4-41c9-90fa-7fbed59c03be",
   "metadata": {},
   "source": [
    "# Part 4: Spark ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9a4777-23c9-43c2-9364-5159874c2480",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc3750f-cec4-4681-8dcf-e784bf08defd",
   "metadata": {},
   "outputs": [],
   "source": [
    "today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a1e6ca-5a96-4f9a-b5c5-89a653c48d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4f52bc-9a9a-406e-bc96-88ae5f8524cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.read_parquet('today_features/part-00000-08f2e6dc-9c9a-4945-b8a3-bac5b9e6105d-c000.snappy.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ade8d3-e201-4794-b3d7-1e12ad840898",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
